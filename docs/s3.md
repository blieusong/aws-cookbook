# S3 (Simple Storage Service)
## Good to Know...
S3 is more a key / value storage where the value is the content of a file. 
There's no such concept as folders in S3. But this is emulated by using prefixes
containing `/` as folder separator in the key name.

## Deleting an object
With `aws s3`

```shell
aws s3 rm s3://bucket_name/object_key
```

With `aws s3api`

```shell
aws s3api delete-object --bucket bucket_name --key object_key
```

## Listing all older versions of all objects
It is highly recommended to save the output in a file as it can be huge and long
to obtain on large buckets.

```shell
aws s3api list-object-versions \
    --bucket bucket_name \
    | jq '[.Versions[] | select(.IsLatest == false)]' \
    | jq -c ".[] | {Key, VersionId}" > bucket_older_versions.json
```

### Filtering on a specific prefix

Use option `--prefix`:

```shell
aws s3api list-object-versions \
    --bucket bucket_name \
    --prefix 'prefix/' \
    | jq '[.Versions[] | select(.IsLatest == false)]' \
    | jq -c ".[] | {Key, VersionId}" > bucket_older_versions.json
```

### Delete all older versions of all objects
Assuming we ran the above commands and saved the output in a file named 
`bucket_older_versions.json`, and using 20 threads:

```shell
cat bucket_older_versions.json | parallel -j 20 --linebuffer '
    key=$(echo {} | jq -r ".Key")
    versionId=$(echo {} | jq -r ".VersionId")
    aws s3api delete-object --bucket bucket_name --key "$key" --version-id "$versionId"
'
```

## List all objects from only the root folder of a bucket
```bash
aws s3api list-object-v2 \
    --bucket bucket_name \
    --delimiter '/' \
    --prefix ''
```

## Computing size of objects in a bucket
First generate a full listing of all objects in a bucket and save it in a file.
```shell
aws s3 ls --recursive s3://bucket_name > bucket_name_listing.txt
```

The advantage of pregenerating the listing is that it can be used multiple times
without having to query the S3 API (and get charged) again and again. It is also
much faster to process a local file.

The downside is the initial time to generate that listing and the fact it may
not be up to date. So use that approach mostly for very large buckets.

### Total size of all objects of the bucket
```shell
awk '{sum+=$3} END {print sum}' bucket_name_listing.txt
```

To get the size in MB, divide by (1024 * 1024). For sizes in GB, divide by 
(1024 * 1024 * 1024).

Total size of all objects in GB:

```shell
awk '{sum+=$3} END {print sum/(1024*1024*1024)}' bucket_name_listing.txt
```

### Total Size of Objects in a Specific Folder In GB
```shell
grep 'folder_name/' bucket_name_listing.txt \
    | awk '{sum+=$3} END {print sum/(1024*1024*1024)}'
```

## Exploring a Bucket's Access Log.
If a `bucket_name` bucket has access logs enabled, and the logs are stored at
`s3://bucket_logs/bucket_name/`, then it is possible to query them with an
**Athena table**.

Create the table with the following SQL
```sql
CREATE EXTERNAL TABLE bucket_name_access_logs (
    bucketowner STRING, 
    bucket_name STRING, 
    requestdatetime STRING, 
    remoteip STRING, 
    requester STRING, 
    requestid STRING, 
    operation STRING, 
    key STRING, 
    request_uri STRING, 
    httpstatus STRING, 
    errorcode STRING, 
    bytessent BIGINT, 
    objectsize BIGINT, 
    totaltime STRING, 
    turnaroundtime STRING, 
    referrer STRING, 
    useragent STRING, 
    versionid STRING, 
    hostid STRING, 
    sigv STRING, 
    ciphersuite STRING, 
    authtype STRING, 
    endpoint STRING, 
    tlsversion STRING,
    accesspointarn STRING,
    aclrequired STRING)
ROW FORMAT SERDE 
    'org.apache.hadoop.hive.serde2.RegexSerDe' 
WITH SERDEPROPERTIES ( 
    'input.regex'='([^ ]*) ([^ ]*) \\[(.*?)\\] ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) (\"[^\"]*\"|-) (-|[0-9]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) (\"[^\"]*\"|-) ([^ ]*)(?: ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*))?.*$') 
STORED AS INPUTFORMAT 
    'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
    'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
    's3://bucket_logs/bucket_name/'
 ```

Then looking for logs is straightforward, but there's a little caveat. The
`requestdatetime` field is not in a `datetime` friendly format. To convert it,
use:

```sql
parse_datetime(requestdatetime, 'dd/MMM/yyyy:HH:mm:ss Z')
```

For example, to get the most recent GET requests on the bucket:

```sql
SELECT 
    *
FROM
    bucket_name_access_logs 
WHERE 
    operation LIKE 'REST.GET.%'
ORDER BY
    parse_datetime(requestdatetime, 'dd/MMM/yyyy:HH:mm:ss Z') DESC
```
